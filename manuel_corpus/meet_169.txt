C :  And it's always the other way around , that's how you put it on .
B :  Okay .
C :  I think the the most of the bad words are at the beginning when the people try to put in on .
D :  At this precise time , yeah .
B :  So you start I think , right ?
A :  Okay .
A :  Hmm so we all attended Interspeech , no ?
B :  Yeah , so we are going to talk about the papers from Interspeech . This is not a good question , I think . All of them , of course . All of them , of course .
A :  So how many sessions you attended ? I mean did you attend any any session ?
C :  Huh .
C :  Whoo .
D :  Uh you can't attend multiple session at the same time , so .
A :  Yeah . That's really that's really your So any general impression or feedback ? So f for any of you it's a first conference or maybe for ? No , I mean in the sense that it's uh Interspeech is really big conference for speech , so did you attend before ?
B :  Oh yeah , that's true .
C :  Ah , I cannot put it .
B :  Okay . Bad answer , right ?
C :  Oh no no , not at all .
B :  Like
B :  Yeah , I it's not every year , right .
C :  Well Geneva , that's why I'm here at IDIAP , because I attended Geneva Eurospeech .
A :  Uh , ok ok yeah , o ok yeah , so then you're hiding a you have attended before yes . Yeah , I see see . Yeah , yeah .
D :  Hmm .
B :  Yeah , we met in also in I_C_S_L_P_ , right ? The first time , so . Yeah . So how how did you like the conference anyway ? When in compared to to th the others , the previous one .
D :  Mm-hmm , yeah in .
A :  I t I
A :  Uh I think it's quite interesting , but only annoying thing is this multiple sessions . And most sometimes you can't able to go to oral talk this oral presentations most of the time , because because like posters is you can spend a lot of time posters looking at m many posters than sitting twenty minutes for one oral presentation . So in t twenty minutes you can see at least two , three posters , and you can directly talk to people , so . Even I found like uh very few people in some oral presentations . I think most of the people are like their own posters only .
C :  I c
B :  But
B :  Yeah .
D :  Hmm .
C :  Why , why you cannot ?
D :  Yeah .
C :  Mm-hmm . Exactly .
D :  Hmm .
D :  Yeah .
C :  Meet
B :  Right .
D :  Hmm .
C :  Okay , so you attended mostly posters , I guess . So how many presentation you you
D :  But yeah .
A :  Yeah , how presentations are hardly handful of like f five or more . Yeah .
D :  Yeah . I liked the invited speaker um about the implant in the ear . It's quite good , no ?
B :  Oh yeah . He was from Australia , right ? The guy yeah . Uh he's very famous I think for that , right ? He's the the inventor of that implant and for for the ear I think .
A :  Ah , ok .
D :  I think so , yeah .
A :  Hmm yeah .
D :  Yeah .
A :  Ah , okay .
A :  Yeah . Even some panel discussions on that human speech recognition reducing gap between the A_S_R_ and and H_S_R_ . It was a bit interesting , lot of arguments and
D :  Mm .
A :  Yeah , b differently it's mainly the differences different approaches of engineers versus linguists or phoneticians and yeah mm .
B :  S yeah .
C :  I realised it were that there were panel discussions only at the place where I could see the the t t you know , there were some boards saying that there is some panel discussion , uh but I I couldn't get no one knew where is it , what it is , where it happens and why and so forth . It was com
A :  Ah , okay .
B :  Mm .
A :  Yeah , even the panel discussions w I think one is really held in small room , so people were really crowded .
C :  Yeah . Finally I found , but
D :  Hmm .
C :  Well , this is not this speech is not restricted only to papers and these things , you know . So how did you like Lisbon then ?
D :  Hmm .
D :  No quite good uh . A bit hot still , but the bea the beach nearby is quite nice , yeah . You should try it , I don't know . Did you go around ? Before or after , yeah .
A :  Yeah , it's
C :  Yeah .
A :  Yeah , .
C :  Yeah , w we went to the Costa Caparica there , with the bus . We we were we were supposed to take take the direct one , but we didn't .
D :  Okay .
B :  So you went to a beach ? To a like Atlantic sea coast or yeah ?
D :  Yeah , 'cause I
D :  Yeah , I stayed I stayed two days before , so uh I went you ju you just take the train , maybe that's the same , I'm not sure . Train north . Half an hour train , yeah . Yeah . West or north , I don't know uh .
B :  Uh-huh . Uh it's not so far from Lisbon , half an hour , mm-hmm .
A :  Yeah .
C :  North ?
B :  Uh northwest mm .
C :  Okay , so you stayed at the at the same coast , not not you didn't go to the island , cross the river where where is the bridge and and then okay , so that was a different place .
D :  No no no , not very far . No no . It's different , yeah .
A :  Ah ok we went to like far south , to Lagos and yeah , that wa uh ha yeah . That was really good , those beaches are really good .
D :  Yeah .
C :  Yeah , with Hamed uh Hemant and these people .
D :  Yeah .
D :  Hmm .
B :  Yeah .
C :  The more south to th the hole through the water .
A :  Yeah , even dolphin in the yeah . But the weather was really hot , the south it's more than thirty five . Uh . Lisbon was good , um little bit bit mm .
B :  Yeah . But I think Lisbon those days are pretty good , like seem to me quite not cold but still okay , like reasonable .
A :  Yeah , even local transport , it's it's . But yeah , most of the time the buses are really crowded . Uh .
B :  Oh yeah .
D :  Hmm .
B :  Yeah , I was happy to be back in Suisse after few few days it's better to be here I think . Yeah , it's big city , many people and
A :  Yeah . Yeah .
C :  Huh .
C :  I was
A :  Yeah , because it's really big city , no ? No like .
C :  I was very surprised that even the trams did have a A_C_ there . I couldn't see it at at all . Yeah , r right , but can you imagine something like in ? Uh it's A_C_ .
B :  Yeah , yeah .
B :  Those new only , right ? Not the old one , but yeah .
B :  Yeah . Ah it's not necessary to have it , maybe . Some time , yes , but not so many days or
C :  Well , it it was much better if if if you would have to spend a hour and a half that uh Hemant and the others spent while going with a ca uh with a bus to to the conference centre from from the the hotel that we were . There was a direct bus and it took them hour and a half then . Mm . Yeah .
A :  Mm . Yeah .
A :  Hmm .
B :  Oh yeah , yeah , that's true .
B :  Conference .
A :  Yeah . Yeah . No , re yeah , but that's that's not good , direct bus is not good like . So you can go to the direct uh ce central place and then occas uh . Mm .
D :  Well y
B :  Yeah , yeah .
B :  It's
D :  We can take the tram and a train . It's it's all conditioned . Yeah .
B :  Better to ta
B :  Yeah .
C :  Yeah , that was the right choice , but to take something which which was uh direct , in quotes . It was n not the f direct as anything else , but
A :  Yeah , da
B :  The easiest way .
D :  Ah .
D :  Mm .
A :  Uh what is the place , uh the the central I mean where we change the bus to yeah , in the yeah , maybe you can check the booklet . Yeah it's there , like I used to find bus numbers from book only .
D :  .
D :  Ah , sorry .
B :  I don't remember the name . Is it there ? Yeah , i it's supposed to be there .
C :  .
D :  No no no it's uh or s I'm not sure .
C :  Uh it sounds sounds like .
D :  Yeah .
B :  Oh , you've got the s
D :  Yeah , originally I brought it because I marked some papers , but yeah . Onl only o one which surprised me a bit , it was uh speaker recognition I think , but it doesn't really matter . They were trying to model uh um covariance of different components for G_M_M_ , instead of using a diagonal . But then if you use a full it's too much . So they use some approach where they tie them automatically and uh after that well , it's more like tying it is still full , but different um components are tied , different components of the matrix are tied .
C :  Let's go on , this was just a
C :  Of what ?
A :  Oh , okay .
B :  Yeah .
A :  Oh , okay .
C :  Pruning of covariance matrix .
A :  It's like uh in
C :  But what what does it mean tied then ?
B :  Uh-huh .
D :  Equal .
B :  Like
C :  Uh yeah , but then you have to store the whole matrix and plus some extra information what is tied to what or b
D :  Uh
D :  No no , you have to store a minimum number a reduced number of parameters , and they are tied in a linear way actually , sorry , not equal , but it doesn't
B :  No , I don
B :  Right , mm-hmm .
C :  Yeah , but
A :  It's like uh all the semi-tied covariance matrices or yeah . So did they reduce it's in between the direct covariance this is full covariance . Should try to reduce some parameters by tying .
D :  Yeah , exactly , that's it uh . So they ju yeah .
C :  Uh I don't know about anything like this , but
D :  Yeah .
D :  Interesting thing is that they applied it on speaker recognition , and on the features before they had three different ways to um decorrelate the features , and and this they showed it uh without this , with diagonal G_M_M_ or with this , and with this , with uh semi-tied covariance matrices , it was a less much less sensitive to uh which um decorrelation procedure you choose . If y so .
C :  Tying .
A :  Oh .
B :  Yeah .
A :  Uh .
A :  Ah , okay .
B :  So they obtain better results with that finally or
D :  A a also better , but of course it depends on the number of of parameters you put
A :  Yeah . Mm .
B :  But
C :  Mm .
B :  Right . But if you compare no , if you compare it with some baseline , let's say you use just G_M_M_ with diagonal covariance matrices
A :  In your tie-in .
D :  Yeah .
D :  Yeah . Yeah , I I think I think it was even slightly better , yeah .
A :  No , I think they got good results with just using diagonals or definitely yeah . Yeah .
B :  A and no decorrelation before like uh
D :  Maybe I can find it .
A :  Mm . Maybe decorrelation again they do D_C_T_ or K_L_T_ or they do that ? L_D_A_ or like
B :  Yeah . Or L_D_A_ is there or
C :  But ooh the the main point is to to uh like make it faster , the the the decoding , or what ?
D :  L_D_A_ , yeah , L_D_A_ .
D :  Make it possible to use uh a full covariance matrix .
B :  Yeah . But but there must be the sense , right ? Because once you decorrelate the data , then you don't need to have full I know , that's so that's that means that the decorrelation is not uh optimal , right ? So so that's the reason .
A :  Yeah . So
C :  Well that's possible , why why it should be i impossible ?
A :  No , but still there is there is still ev that's why like people come
D :  Yeah .
A :  Yeah , definitely it's not optimum . So it's it's also like doing along along the diagonals all again .
B :  Mm-hmm .
C :  So it's completely impossible to do it with full covariance matrix ? Why ?
A :  It's really computationally really uh yeah so see suppose if thirty nine in te thirty nine to thirty nine for every yeah , yeah , so
C :  Yeah , computationally e expensive , but
B :  Yeah , yeah . It is , yeah .
C :  Of course , from the point of view of computation , but otherwise it's there's no problem , no ? with okay .
A :  Uh there is a thing
B :  Uh no , there there is not
A :  Yeah . Ne no problem , but if you see so many models and so many mixtures yeah . But
B :  Yeah .
C :  Okay , so so here you have to choose either many many things to store and huge computation uh
B :  Right .
A :  Yeah , thirty you can see you c you can't really like even you can't the models itself is like thirty eight times more than that , so . So y if you have like five megabytes or ten megabytes of models then yeah . But even I think it's really bit like impossible for the really big systems I guess , like storing storing itself .
B :  Right , right .
C :  Yeah , yeah . No . Uh depends .
B :  Yeah , yeah .
D :  Yeah it's this one .
C :  So .
B :  Yeah , yeah , definitely . It's almost impossible for like L_V_C_ aside , impossible to use it , right ? You've got dimension thirty nine times thirty nine ? Yeah ? So yeah . It's impossible to use it . And you cannot even train it , right ? More or less I think .
D :  It's i improved covariance modelling .
A :  Yeah yeah , yeah yeah , it's really ti
A :  Yeah yeah , for every you're so you're to store all this thirty
D :  Yeah .
C :  So there are no results there ? No
D :  No , but in the proceedings you can find them . They are better , yeah . They tried P_C_A_ , L_D_A_ , M_L_L_T_ .
A :  Yeah .
C :  Hmm yeah .
A :  So from
A :  Mm-hmm . Mm-hmm .
B :  Oh , so that's interesting .
C :  Uh-huh .
D :  I i it's not really anything new , but uh the mm they just applied that to speaker I_D_ . Yeah . Yeah .
A :  Speak uh yeah , different task and
C :  Yeah .
A :  So uh I found one paper interesting . I think you know Vivek most of you know . Uh this is uh Vivek's paper on like variable scale feature extraction . Normally we use to fix it window for feature extraction , like twenty five milliseconds or thirty seconds . Here he's proposing variable scale , because the fixed scale is non-optimal for it's non-optimum because like for vowels you can have m much longer , and for plosives and these things you really shorter , like even around less than twenty milliseconds also . So he's ki proposing like this variable scale window f uh kind of online for each segment .
C :  Yeah .
B :  Mm-hmm .
B :  Much longer .
C :  Mm-hmm .
B :  Shorter , yeah .
C :  Okay .
C :  Why it has to be online ?
A :  Yeah mm mm you can do like you can measure and you can do but he's mm basically doing some mm likelihood ratio testing . So if so the main idea is like suppose you have uh one segment , so he's trying to find the stationality cautious stationality of that segment .
C :  Okay .
B :  Right .
C :  Like extended as as more as much as possible to keep the okay .
A :  Yeah , as much as possible like but yeah , definitely you have to assume some l uh like minimum and maximum sizes of your own , so he is using minimum of minimum is uh I think twe I don't twelve point five milliseconds I think . Maximum is uh sixty milliseconds .
C :  Of course , of course .
B :  Wh what is the minimum and maximum ?
B :  So it's not so mm
D :  I isn't it smaller , the minimum , no ?
A :  So the minimum
B :  Two point five milliseconds .
C :  Whoo . Yeah , it's right .
A :  No no , I think a minimum is twen twenty milliseconds is i yeah . Twelve point five milliseconds is kind of sift . Twenty millis minimum is no no , de because of uh
B :  Twenty milliseconds ? From
C :  This is maybe because of F_F_T_U_ or these things . It's no not as small as as it should have been , but mayb maybe because of other processing after . No ?
B :  Oh , yeah .
D :  I don't know , I just
A :  No , you can use mol yeah . No , mainly because of uh M_F_C_C_ computation , because yeah , it becomes really noisy , like if you s yeah , ten milliseconds means you have only eighty samples for uh and then you have refused twenty four filters , the filters won't get any samples , so .
C :  Yeah . Well noisy , yes .
D :  Mm .
B :  Right . Sure .
D :  Mm .
D :  Yeah , okay . But it's not uh an intrinsic limitation , it's just because of ri yeah .
A :  Yeah , because of computation . So even I I think even st uh he can find less than twenty millisecond windows also , especially for plosives uh .
D :  Okay .
B :  And the shifting is still the same , or not ?
A :  Shifting yeah , he's using twelve point five millise yeah , shifting is almost same . But the problem like
C :  Overlap you mean .
B :  O yeah , yeah , overlap .
B :  So
C :  Uh w how do you mean to say ?
B :  Like um no , how many frames per per second you do have ? Is it again like uh one hundred frames , or it's less or even if you keep variable length of the frames , right ? You can still keep the same frame rate .
C :  Uh
A :  He
A :  Yeah . Yeah , he's keeping same number of frames . Uh he's using uh twelve point five yeah , twelve point five millisecond
B :  Because I understand this would have sense for , I don't know , speech coding where you want to preserve uh or you want to encode it into less frames , right , but why do why to do it for speech recognition ? It means that uh
A :  Yeah .
A :  Number of uh frames ? W you want to
B :  Yeah , no , wha why to keep variabl variability in the length of the frames ? Wh why not to keep the same length , what is the
A :  Actually the problem's again uh uh you see the shift is the Nyquist frequency if you compared modulation spectrum . So again , if you change this shift , the mod the Nyquist the modulation spectrum , Nyquist frequency changes for each window . So if you want to do again another high-level feature extraction again , it will be problem , so . So but he's uh like he's he discuss I mean describing this , because even this itself is a problem , keeping the f uh fix to frame size , because you're analysing your this uh segment many times .
B :  Yeah .
B :  Right .
B :  Right , right , right .
A :  So this shift will suppose if you find one segment of sixty milliseconds , and then you're doing this ten millisecond so every almost I think yeah , mm uh yeah , five frames y you're analysing the previ this already segment so . So again , this may blur some frequency transitions or yeah , but the problem is again this uh bottleneck is here , like you can't change your Nyquist frequency modulation and
B :  Mm-hmm .
B :  Right , right , right , ri right .
B :  Mm-hmm . Mm-hmm . Mm-hmm .
B :  Right , yes .
C :  Um I have two questions , first is essentially how he's doing in that . No , well uh once he gets the frame , like from here to here , then where he advances to to start with c computation of the next frame ?
A :  Yeah .
A :  Yeah . Yeah .
A :  So it's so uh so the main idea is like first to take the some some window , some sam some samples , then he assume somewhere some variable and point there is a change .
C :  Yeah , mm-hmm .
C :  Okay .
D :  So it's not n it's not symmetric necessarily ? 'Kay .
A :  Then then it's not symmetric , i you can cho uh y it's basically uh too like again , do for all the samples in that gyro . So then what he does is like he proposed uh like some like ratio test based on maximum likelihood . So th tha that is really simple , like so what he does is like so he he computes the residual , he first he does the L_P_ analysis and then he computes the residual , and he takes the residual energy of f full signal like gy gy gyro samples .
C :  Okay .
C :  Okay . Mm-hmm .
C :  Fu ok okay , so do what do you mean full ? Some some sub-window or the the whole ten seconds of speech or what ?
A :  The full signal . So maybe I can
A :  The wh the whole window .
A :  No , that's that like he assumes yeah , sixty milliseconds or something he start with , so so then he can yeah , yeah . Yeah , then he can split that window at point N_ say point N_ , so then you will have gyro two no , you can segment it maybe .
C :  It has to be also some somehow
C :  Okay , something longer window than the one we are speaking about , okay ?
C :  Uh w
C :  Okay , so you have a window and then you split it to s two parts and you shift the the point where you split it or okay .
A :  Yeah , so yes , he's supposed this is your p window . So then you can move your like this . You can move your point , so that then this will be like one and this will be second . But again , he assumes some initial uh sizes for these windows .
C :  Okay , splitting point , okay .
C :  Okay .
C :  Okay , so basically we have two fixed points .
A :  Yeah . He don't start with the yeah , he don't s yeah , start with gyro sample or something . He's starting with uh so the l left window is starts at twenty milliseconds , so so you already hear like s suppose this is full signal , you're already here . So right window is should end at twelve point two , so you're to search basically in this range . So
B :  Right , yeah .
C :  Yeah , okay .
B :  Mm-hmm .
C :  Okay .
C :  Okay , so once you have say one hour of speech , you start at the beginning , and first you try with twenty milliseconds . Then you shift to twenty five and so forth until you
A :  Yeah .
A :  Yeah .
A :  Yeah , and then if you the maximum is sixty millisecond frame for him . So if you don't find anything
C :  Okay , and and you find the point where where what ? Where where do you stop actually ? Yeah , mm I know , but you start at twenty and go to sixty , but where which point do you choose ?
A :  Sixty milliseconds . If you d
A :  No , the point is depending on the likelihood ratio . So he got so he supposed this so this is suppose se segment one , and this is segment two . So he compute this error here . So and then he p gives some kind of in terms of this residual letter . So this residual letter is for full gyro samples . Then there would be suppose this point is say N_ . So gyro to N_ and then like
C :  Uh okay , but which which likelihood ratio ?
C :  Okay .
A :  So this the b basically the main he questions or here
D :  So he's maximising that or mm
A :  Yeah , he's for finding the likelihood for full frame , and then it's basically likelihood ratio test , so he's comparing the likelihood of the full segment divided by the likelihood of the the sub-segments .
D :  Mm . Mm-hmm .
C :  Yeah , but uh likelihood , that's something and something . W what is likelihood ?
A :  Likelihood of the uh like this likelihood is uh this error estimate of the residual .
C :  Okay .
A :  So residual power he can compare t so he after do after doing L_P_ he can get a l residual and then he can compare the power of the residual . So that he's proving that power is again maximum likelihood estimate of your L_P_ parameters . So that which interesting it's you don't really need to do a lot of computation , you just need to take the error of n uh residual of this full window and then residual of energy of the sub-windows and then you can just divide them and then the only thing is like again he has to use some threshold to decide , that that's only the p problem .
C :  Yeah , but it has to yeah .
C :  Okay .
C :  So it's basic
D :  Hmm .
D :  Yeah .
C :  So
B :  Right .
C :  Mm . Yeah .
C :  Yeah , but still . If
B :  Mm-hmm .
A :  He got uh he's founding something more uh around three or three point five E_S_T_ optimum threshold , but the and another advantage is like it's not really changing speech recognition , whatever , so not really changing because of the threshold , so . Uh you don't really need to f like fiddle with threshold a lot , so mm .
B :  Mm-hmm .
D :  Mm .
C :  Mm
D :  Okay .
C :  Yeah , but why do the main value which are comparing to , the the the overall value or overall likelihood or whatever you call it , why he's using the whole signal for this ? Because so if the whole signal was uh steady somehow and it it would be able to be modelled by L_P_ model , well , then the residue will be very low . So he's supposing that the whole signal won't be able to to be captured by L_P_ model and that the residue would be high enough or ho wh why he's comparing the o to to the overall ?
A :  Yeah .
A :  Yeah . Yeah .
A :  Yeah .
A :  Yeah .
A :  Yeah , this is yeah , I understand . This is this is the actually basically some theoretical proof , like maybe if you want threshold you can see and he's coding from uh Leven like uh . No , this is from again yeah , statistical signal processing . What they say here is like suppose if you analyse two distinct or uh this L_P_ analysis in the same stationary analysis window , the coding will always be greater than the ones resulting from this analysis in two windows . Two stationary windows .
B :  Mm .
C :  You know , I I don't see the point of dividing over the the whole stuff .
C :  Y well , sounds sounds reasonable .
D :  Mm-hmm .
A :  So the he's basically based on this theorem , so so what he's saying is like if you do this error , it will be always greater than the mm th these errors of two stationary windows .
B :  Mm-hmm .
C :  I see , I see , I see .
B :  Well
B :  Mm-hmm .
C :  Okay , that's reasonable . Because actually if it if if uh the whole signal would be able to be modelled by L_ uh by L_P_C_ then anyway he has to design some some framing . So even if it's very stationary by d um by dividing over this whole stuff , he is able to find some reasonable boundaries . Um m ma makes sense um .
A :  Yeah .
B :  Yeah .
A :  Yeah .
B :  Yeah , probably .
A :  Yeah yeah yeah yeah . Yeah , this maybe I think yeah , this he's getting some improvement , but
C :  Okay . And
D :  Yeah , ho yeah . How did you cope with what you mentioned some time before , that if it's a very long session for each twelve point five frame you will actually get the same segment now . So how d
A :  Yeah yeah yeah yeah . So he's just uh uh he's normalising the energy coefficient , because energy is really affected a lot . Uh so he's li
C :  Mm .
D :  Ah , okay .
C :  That has to be power , not energy definitely , no ?
A :  Energy like , because C_ gyro component like , so he's using M_F_C_C_ so he's normalising the C_ gyro component for this t yeah .
C :  Yeah , but it has to be normalised . By the length . Then power , instead of energy . Energy over length is power .
D :  Mm-hmm .
A :  Power is again like yeah yeah yeah yeah , it's time like yeah . It's okay like samples , or you can see root mean square energy or something .
C :  Yeah , yeah . Duh duh duh duh duh . Okay . So but this so two things I still don't understand , thi this is supposed to be only for M_F_C_C_s and these easy features ah , one question I I wanted to ask before , does does he preserve within the feature vector the framing he's using finally , because does he somehow put the information about was the size about the frame to the feature vector itself or he's throwing away this framing ?
A :  Yeah .
A :  Yeah .
B :  No , I don't think so .
A :  No . No , n no , you vo what you mean like the window lent . The sixty mil no , it i it doesn't really
B :  There is no information .
C :  Something like this , because the recogniser could be good for the recogniser to know what was the chosen framing by this extraction .
D :  Mm .
A :  Uh but I think it's he he doesn't use I guess , because uh uh then like you just you take these features and you train model , so . It really matters wh what frame shift you're using for models , like because how many he needs to use same freq
B :  Hmm .
C :  Okay .
C :  So so it n uh it might be bad for then for the back end , it might screw up the the t speech rate or normaliser .
A :  No no , wait . No , he will get like every ten milliseconds he get one fra one features , like he it doesn't really depend , because suppose if you take your t case , like you your window i maybe longer , so doesn't really matter like how what size window you use or not . But I tell uh at every ten milliseconds whether you give some features or not really matters , no ? Like so . So it doe you can use uh thirty milliseconds or fifty millisecond window .
C :  Huh .
C :  Oh , probably I didn't get it too , so the framing is actually equi-distant . Just the windows are wider oh ah , I see . Because it would be crazy , yeah , a little bit .
A :  Same . Yeah yeah yeah . Yeah yeah yeah yeah yeah . Yeah yeah , framing it's like it's really yeah . Yeah , if you have if you're change framing , it's really I think even it's really problem , no ? Like if you try even models also I guess , if you change . It's not only with this model , it's in spectra and shifts in deltas , because
B :  Yeah yeah yeah , right , right , right .
B :  Yeah , yeah .
B :  Yeah , it might be difficult .
C :  Oh I see , so so it's it's like this .
B :  Mm mm-hmm , probably . Who knows , maybe it's possible to use it somehow , but then there is uh information about temporal information is somehow included in such yeah .
A :  Yeah , but it's really complicated I think . If you use yeah , that becomes really
C :  Yeah . I cannot imagine the transformation to get the modulation spectrum out of such a stuff . Uh .
A :  Yeah . But this is kind of interesting .
B :  Yeah . There is also some um like uh not paper , but I saw the algorithm I have been using even that called temporal decomposition , I don't know if you kn you know that like
A :  Yeah yeah yeah , the temporal dec .
B :  Uh I don't know who propose it , I just had it from from BEAMbot for like a BEAMbot , do you know the guy ? He's French um somewhere now in teleconference like mm I don't know where he's working , but th we were using it for uh speech coding , so we just had a speech and you had decompose a speech into such segments , temporal segments which were stationary inside . And then you can more or less uh quantise those segments somehow and use it for encoding let's say , or and it worked prett yeah , yeah .
A :  Mm-hmm .
A :  Yeah .
A :  Yeah .
A :  Yeah .
A :  Yeah .
A :  Yeah . Yeah , first this is proposed by like uh even we read one paper in our reading group , you remember that ? I have implemented , it's quite working well , like uh yeah . Mm . Yeah . Yeah , Honza is also worked for his be yeah . Uh i yeah .
C :  Mm .
B :  They were using like S_V_D_ stuff , singular value decomposition for that and it worked pretty well , like I was surprised like . Yeah , yeah , exactly , uh he was using that . Yeah .
C :  So why don't you still use it ? If it works so well .
A :  Ne he was re he was quoting
B :  Oh well , it's it has been used for speech coding . Nob nobody use it for recognition stuff I think , I never heard that .
C :  Yeah , but you do the speech coding , so do you still use it ? Why ?
A :  But that
B :  No , I don't of course . Yeah , that's good question . Maybe .
A :  Yeah , he was uh quoting that paper also , like v he was telling this uh this is kind of optimisation criteria what this temporal decomposition is doing like . You're trying to segment your s signal into like discreet windows and then so but he was telling like this uh relationship between optimation and and then cautious stationality is not really obvious . So stationality is again different , no ? Stationality is this is optimation , we want to like see some signal , few segments , which can really represent whole , so maybe that's why like this may not be really
C :  Mm .
C :  Uh .
B :  Right .
B :  Right .
B :  Mm-hmm .
B :  Yeah , that's true .
B :  Mm-hmm . Mm-hmm .
B :  Mm-hmm . That's true .
C :  Uh it's interesting , this .
A :  Yeah . Yeah , this one yeah , he's getting some improvement n uh this is a , yeah .
B :  That's important .
C :  Well there should be after each conference there should be something like ten people should sit down , read the papers , and then rank them . You know , a all the all the all the similar papers within one session say sp uh focus to this topic , and say this is the best , this works slightly worse than this one , and throw away what is not that good and and th the first three three papers which are the best shou should be implemented and used from from that time onwards , like forget the all the M_F_C_C_s , because it's too old , and then start building the story on the new stuff and not 'cause it's
B :  All the papers .
B :  Okay .
B :  I think people wouldn't like it . Many people wouldn't like it .
A :  No no no , they again
B :  Right .
A :  But again uh , getting that ten papers is really difficult task . It takes maybe years and again uh it definitely like who will who will choose the ten papers like , and if we asked yeah , but yeah , those things are practically kind of really but I think this kind of things what you showed I mean what you said is really good , like if people start implementing like some people propose something in feature level , some propose in something in model level , but these two are really independent , no ? When really work combine these two or
C :  Yeah , it is but I know , I know , uh th this is just a theory .
B :  Yeah , yeah , yeah sure .
B :  Exactly .
C :  Because
C :  Mm yeah .
C :  Yes , and I want to yeah .
B :  Right . And everybody's uh like uh using different training and testing data and then you don't know if like
A :  Yeah , especially features , like suppose if somebody come with different good features , again people st again use M_F_C_C_s
C :  Yeah .
B :  They show it works for TIMIT , but then you try to use some different databases , you see it doesn't work .
C :  It looks like
A :  No no , it it's not really sure also , make suppose uh people you can't really force people to use same features , so so they wo they will be happy with their own features and their own scripts , so their bit rate like tend to change features every time and it's
B :  Yeah , sure . Yeah , yeah , you can .
C :  Mm .
D :  Sorry . Mm uh what it's a little bit what NIST is trying to do , no ? So .
B :  Yeah , exactly , yeah yeah yeah . NIST evaluations .
C :  Yeah ?
A :  Hmm .
C :  Yeah . Because to me it seems that everyone is still comparing to P_L_P_s or M_F_C_C_s , and there are so many new systems and everything is new , but it baseline is still uh t twenty years old and
D :  Yeah , bu
A :  But even NIST you can
B :  Yeah , but nobody you know , it's those new systems are still not general , like you know , like those P_L_P_ and M_F_C_C_s , because everybody knows that di it worked somehow , right , for any kind of data . That's why they're comparing to to that , so . Once somebody will come with something new , okay , he he's showing it works for some datas , but still yeah , yeah . First it's difficult to to show that it works for all all the datas , right , because it's really and
A :  Yeah .
A :  Yeah , yeah .
C :  Yeah ?
C :  Uh yeah .
A :  Then
A :  So at lea especially L_V_C_S_R_ in really big systems , then people
C :  No okay , so
A :  Yeah . Yeah . Because yeah , it's it's again you don't know .
C :  So it should be every papers then should contain all the results from NIST evaluation or some standard task then . Otherwise
A :  Oh . No , then uh there will be only one conference in three years or something for speech , because
D :  Yeah , but then we all optimise uh on the same task .
C :  Uh uh that's the other yeah .
B :  Yeah .
D :  So you cannot get anything new out of that after some time .
C :  Okay , so it could be you you would have to publish the results on this standard task , and then you could say well , I also tried on this and this
A :  But again like
D :  Yeah , but then you run out of time . No , i ideally you're correct , uh but
A :  That's so you can publish only once in three years or
C :  Yeah . You know , but then then only it makes some progress , this .
A :  Yeah . But even people are doing suppose in Interspeech there was uh some challenge for speech synthesis , so so what's is uh like it challenge was really good , like sup they give the database , so you have to work on that database . What technique t use is it's your choice like , but that database and then the results analysis is they decide , so . Even speech recognition also some tests are coming , like for phone segmentation or something . People give some database and then you have to dis you can use whatever you want and you have to produce even for features also like features also I think
B :  Yeah , that's your choice .
C :  Mm .
B :  Mm-hmm .
D :  Mm .
C :  Yeah , but can for for example can you use from some data from from the L_T_ world ? For training say ?
A :  Yeah yeah , the they they designed the data such a way that it's it's really like really real uh data .
C :  Yeah , but they give you also training data or only the testing data ?
B :  No no , i just for
A :  No no , for s nay , speech synthesis it's like they give you some data , so whether like you use for training or l you can you can do like data driven or like model based approach or whatever like . At the end then they will ask you to synthesise some sentence and you have to synthesise them and then you have to send them
C :  Oh , i uh since this is only okay .
C :  Okay , so so they give you only the training or development data only ? Mm-hmm . Yeah .
D :  So yeah . Ho how how do they how do they evaluate ? I just don't know .
A :  Your training data , onl no development , training , no yeah , training
B :  Yeah .
A :  Evaluate it's again subject to s uh because for speech synthes yeah . Yeah , listen and yeah d yeah , even uh they it's mostly they use native speakers only , because they can really judge well , so .
D :  So they ask peop they ask people to sit and listen basically ? Yeah ?
B :  I think so , yeah , probably , mm .
C :  Hmm .
D :  Hmm .
C :  Mm .
D :  Okay . But they d they don't have any measure like you have an original speech , you transcribe it and
A :  Yeah yeah yeah yeah . Yeah , yeah yeah . So even they propose some kind of word error rate . So you have the original speech , so i the synthesised speech , is there any words which are not matching ? So even speech synthesis , they're also uh introduced this W_E_R_ term , so . Uh . But this was really quite successful like in this conference , Interspeech . So a lot of people participated in uh I think even they're continuing this for uh next year and
B :  Mm-hmm .
D :  Okay .
B :  Mm-hmm .
B :  Ri I
D :  Hmm .
B :  It's just T_T_S_ , right , text to speech ? Right .
C :  Hmm .
A :  Text to speech . But uh I heard like even for I_CASSP next year there is some com uh like computation by Martin Krug , Sheffield , and on this feature extraction stuff and so at least if you make task simple focus , then it may be good to compare these features and but if you s use some uh large vocabulary system , it takes t six months to build and at the end you don't know whether your features are really uh so . So maybe that's for like people are always using P_L_P_s or M_F_C_C_s , it's it's such a b like lot of time involved , so you can't really check many thi
D :  Hmm .
B :  Mm-hmm .
D :  Hmm .
C :  Mm-hmm .
B :  Right , right .
C :  Mm .
D :  Mm .
D :  So maybe well , but then well no , if somebody like NIST will have one recogniser and you just plug a different feature , but uh it's dangerous .
A :  Yeah , yeah yeah yeah . But at least in IDIAP we have this numbers recogniser is almost kind of free , so everybody is using so that's what we are doing , no ? Almost almost like we are putting different features and I think yeah , yeah , numbers recogniser is not really different from like uh a f at least in IDIAP we have almost same , but maybe you are using twenty nine , that's for you it's different , but one more
D :  Right .
D :  Mm .
D :  Hmm . Yeah . Yeah .
C :  Uh .
C :  Al almost . Yeah . We are a couple of people at IDIAP and there are already two s two different setups .
A :  But at least i twenty seven . Hemant and we're all using twenty seven , so . Yeah .
D :  Hem Hemant is using difference
B :  Are they ? For O_G_I_s ? For O_G_I_ numbers ?
C :  Stories and numbers , or
B :  Different setups ? Like some people use this setup and some people that ?
C :  Yeah . Uh I think we use di distinct train and test set , even se the different sets of phonemes . Different
D :  Seems
A :  Yeah .
A :  No , but the problem is you're using only digits , so maybe that's how you
C :  Well , I don't use only digits . My M_L_P_s are trained on everything and uh test set is digits , yeah .
A :  No , but test set is digits , your m main task is digits k yeah , that's uh like but O_G_I_ numbers are there like now we're little bit convulsed , like we're using twenty seven phones and before it was like twenty four , twenty five . But it's better , like once you have like whatever the back end , then you can put features like whether they're gammas or like spectral entropies or M_F_C_C_s or whatever . Then at least you can see
B :  Uh-huh .
D :  Hmm .
B :  So you just
D :  Mm .
D :  Yeah .
D :  Would be mm would be nice for Aurora maybe to have that . Aur Aurora . Yeah .
A :  Uh ? Yeah yeah yeah . But Aurora is
C :  For for what ?
A :  Aurora task . So but Aurora , is it really big database , or I mean how much time it takes to set up system and then
C :  Ah .
B :  Oh yeah .
B :  No , uh it's . I dunno , I I've been working with that and
D :  No no , but yeah , yeah .
A :  It's fast , no ? But the problem is Aurora again , these models are word based models , no ? Yeah , so again here we use triphones and so but definitely , I d if we want to show noise setting , it's you have to show on Aurora also , like Aurora is real time .
C :  Yeah .
B :  Right , yeah , yeah .
D :  Mm .
D :  Hmm .
D :  Yeah .
B :  For noise conditions , yeah , it's pretty good . Well , th those f recognisers are are made there or just you just use them , that's it . You don't have to play with it . She even is f it was forbidden in the time , right , so you have recogniser , just use it . Don't play with it You just play with the features . Yeah , right , exactly . So .
A :  Yeah .
D :  Hmm .
A :  Yeah .
C :  Yeah , you cannot play with that . Because of the voices wouldn't be comparable then .
A :  Yeah , yeah yeah . But yeah
A :  Yeah , at least I mean wha those studies I think people already did for even in I_C_ I_C_S_L_P_ two thousand two there is special session on features for Aurora , so yeah , yeah , i yeah yeah . This ICSI features and this uh s uh s uh O_G_I_ features and all this . So there are already people but again like uh , at the end like for L_V_C_S_R_ people are using P_L_P_s or M_F_C_C_s or nothing of these fancy feature
B :  Yeah , yeah . Ri right .
B :  Yeah , yeah , I remember . With David Pearce he was .
B :  Yeah yeah , exactly .
C :  Uh .
C :  Hmm . Because who is the one who makes who actually uses what we d try to do here ? These are companies or because it's not scientists , scientists just start from P_L_P_ and then design the new features , new back end , or new anything , but they start from the old stuff . But who is the one who is using these results which we publish and try to make
A :  Yeah .
A :  New
A :  But maybe my have a f for publishing another paper or something .
B :  We are using those results more or less .
C :  Yeah , but once you once you finish your P_H_D_ you go and it's over . And those who don't ? They stay and publish another papers and they but they they don't turn into a life , no ? into life .
A :  Mm .
B :  But not everybody go . Not everybody . Many people stay , so .
A :  Yeah .
A :  But maybe Guillaume , you can tell , no ? Like uh you worked i with these Siemens people or no , Siemens or these Daimler Chrystler . What kind of features do you use , like do you use um any you don't yeah , you don't
D :  Yeah , Daimler , yeah .
D :  I don't know . You know , they're extremely secretive . I had a hard time just to get the signals . So I asked other questions but uh it's quite tough , yeah . Yeah .
A :  Ah ok okay . Okay .
C :  Mm uh-huh .
A :  Ah ok
A :  Yeah , even s yeah . I didn't discuss anything with Sunil also , like I dunno , technical discussions , anything . these kind of things are a bit , yeah . Yeah , yeah . But do you have any like feeling that whether they go for all these new features or they usually use only mm . Maybe yeah .
B :  Yeah .
C :  Because
D :  Yeah , they have to yeah .
C :  I I won
C :  Yes . I wonder which level they are using out of what is there .
D :  They have yeah , they have to do some test in their Mercedes , some speech recognition test , mm but it seems now they mm at least Daimler , they mostly work on dialogue management , uh but not much on features and recognition .
A :  Mm-hmm .
A :  Hmm .
A :  But even they don't believe like with one paper or two I guess , because at least for them no P_L_P_s or M_F_C_C_s , they know that okay , these things work on every task , so okay , we can pu put hands on these things yeah .
C :  Hmm .
D :  No .
B :  Yeah , sure . But exactly .
C :  Yeah .
D :  Yeah . I d I mm . I doubt they are trying right now . I doubt that at Daimler they are trying right now , but maybe other companies , mm .
C :  They grab
C :  Hmm .
A :  Huh ?
A :  Yeah , yeah yeah yeah . Maybe like yeah , f because Sunil is working in Nokia , he's a expert in kind of features or yeah , it l I think . But yeah , we're not sure , even Sunil won't tell us .
C :  Hmm .
D :  Uh .
C :  Okay , so we can hope that Nokia will be using tandem tandem P_L_P_s .
B :  No .
C :  Yeah , I cannot .
B :  Well , many people place with Tandem , right , like M_I_T_ or who is using that some not M_I_T_ , but somebody
A :  No , no n no , this uh Cohen , you mean like uh uh
B :  I dunno , Hynek told told us that somebody's using that um very actively , like okay , it it works for them and and they are close to like industry like uh you know , there is it's a research group of I know , I don't think it's ICSI . Somebody else or I don't know , somebody new uh s but I think some t somebody like A_T_ and A_T_ and T_ or what ?
A :  Hmm .
A :  Oh ok
C :  Huh . So it's not related to ICSI .
A :  Huh .
A :  Uh . But this was signal company , no like ?
A :  This voice signal company , Hy Hynek always mentions uh Cohen's company with what they're using , you n yeah .
D :  Cohen ?
B :  Oh qu yeah . You me I don't know , he's even if Qualcomm ? No . Now y oh yeah yeah . For right , right . No , I don't know if it's
D :  Jorda Jordan Cohen .
A :  Voice no no , voice signal . They make uh recognition for these Samsung phones and yeah . Uh they have like really s many recognisers I guess . Maybe definitely they may be using P_L_P_s or you know .
D :  No no .
C :  Yeah , but it was
C :  Mm .
B :  Yeah , I think so . Yeah , definitely . Maybe they are using P_L_P_s and but I th I believe that there is still some stuff up to yeah , for RASTA or whatever else , like all those decorrelations and transformations . I think it's there , but more or less it's again based on M_F_C_C_s or something like that . Or maybe traps .
A :  Yeah RASTA or RASTA or something you mean . Mm . Yeah , because mm .
C :  Hmm .
C :  Hmm .
C :  Hmm . Because they grab whatever is certain to work , and they they use it . They don't use anything which is to uh too new which is not that safe .
A :  Yeah , but again
A :  Yeah , too new is really definitely because like want to at least see i at least four , five years of results of some new features and some consistent results or
B :  No , more or less I think yeah .
C :  It's like a it's like a NASA , you know , they use four , eight four , eight , six , machines to to to to put two n uh space ships , because it's it's known to work perfectly well or Pentium , not any P_ four or whatever , because it's safe , it has been working for twenty years , and then only it safe to put well not twenty but it's always yeah yeah , of course .
A :  Ah . Uh space or ah , okay .
B :  Right .
A :  Yeah yeah yeah . Mm .
B :  That's true , it's true . But that's different situation , because right ? They need that it doesn't create that much , yeah . If something happened with the machine and uh I don't kn the alloc system , it doesn't matter , right ? Somebody will fix it and change it , but
A :  Yeah .
C :  Of course .
A :  But yeah , d
A :  Yeah , uh another issue is like again the computational issues , no ? If they want to make on mobile , they don't want really use some fancy , really expensive computationally expensive features or all these
C :  Mm yeah .
B :  Sure .
C :  Mm .
D :  Mm .
C :  Hynek also told about this Cohen ce cell phone , which he was showing to to work l uh with the L_V_C_S_R_ . He he was saying that there is something really really uh simple , the decoder I mean , not the features , but he was mentioning something to the decoder . This is just there is almo
A :  Yeah yeah , it
B :  Yeah .
A :  It's just phone recognition or something like
B :  Even even not Viterbi , just even something simpler . Yeah .
C :  Yeah , something like no D_T_W_ or something , I don't know , I'm not really sure , but he mention that
A :  Yeah .
A :  But most of the time mobiles , they use D D_T_W_ kind of thing , no like ? All these uh so voice calling yeah , voice calling and these thing , because that's easy like , because they don't really need to put lot of uh memory and these things , otherwise if they want to build a real
B :  Uh I don't know . Do you think so ? Yeah ? Oh yeah yeah , yeah yeah , that's true . Yeah , sure .
B :  Yeah .
D :  Mm .
D :  But there you could send an S_M_S_ I think , or something like that . Uh you could dictate text , so . On a phone , yeah .
C :  Hmm ?
A :  Yeah yeah , yeah . Where where ? Mm . Yeah yeah .
C :  That was impressive .
B :  Even on the phone . N some new one or
D :  The phone that uh Cohen yeah , voice signals .
B :  From that C Cohen's ? Oh , uh-huh .
A :  Yeah , Cohen , yeah , yeah .
C :  Yeah , but it's that's a similar one to what Herve has , for example some he was showing it uh live on M_L_M_I_ c last M_L_M_I_ conference . You can look at the video and play it uh .
D :  Yeah .
B :  And but which uh phone it is ? F Samsung , something ? Where uh some but some which you can buy , right ? It's
A :  It's Samsung , some some new really new phone like , mm .
C :  It uh I'm not sure .
C :  Yes , commercial phone .
A :  Yeah . But uh the software is always available or like
B :  It and you can train it , like only your speech or it's general uh it's quite interesting , yeah .
C :  What do you mean , always ?
A :  No , the software is again on top of mobile , or like it comes with your mobile
D :  I don't know , that's
C :  Well , they they just bought a mobile , and based on the O_S_ it's using they they they developed the software .
A :  No , the thing is usually these kind of these kind of facilities are like no not normal , so so they may charge more or like yeah , yeah . Yeah . Ah , ok yeah , definitely they may charge for more money f again , some extra bill for this using this recognition engine or something .
C :  No , this is the development phone , this is not the commercial stuff with the software . They just use the device .
B :  Okay , uh-huh .
B :  Or you you can just buy that application , right ? I don't know , like it's like you you've got some mobile phone , and then you can buy from another company some such application wh you can dictate your S_M_S_ or whatever .
A :  Yeah .
A :  Yeah .
C :  By by what ?
A :  Yeah .
C :  I don't know if it's already the case right now .
B :  S I don't know if it is done now , but like
A :  Yeah , but maybe like if it can't really recognise you it will be really annoying it's better .
B :  But have you ever used like dictation system for Windows ? Those uh there were many of them . Yeah , drag the mouse there . And I heard that it worked very very well . Um
A :  Yeah , I have used once like this dragon , that was kind of good like , yeah . Because even in when I was working in uh like in Edinburgh like , one guy , he use i he got some problem with hands and then he was always using this n uh dictation even he used to write lot of C_ plus plus code with the dictation like . It's really good like , so .
B :  Alright .
A :  But you ought to really train and then you're to kind of uh yeah . I get used to the system . Yeah yeah , it's really like you ought to get used to the system , like you how to say all the callings and this thing system how t but it's really funny like , he is able to use it for like like years , I mean he used to write a lot of code and , you know , it's really great .
B :  Right , newer speech , okay .
C :  You had to get used to the system , not the system to you .
D :  Okay .
B :  Okay .
B :  How to say okay .
C : 
B :  Mm-hmm .
C :  It's nice . By the way , you should be the one to ask is there anything if I'm just a p private person and I want for free any software which is able to read me a book in English , uh for example that Festival , am I able to put the text to the system and to in real time to listen to the output ? Festival is able to to to read just a mm normal plain text in reasonable comprehensible English or so that I can understand it ? Non-native speaker ?
D :  Hmm .
A :  Yeah yeah , you can listen Festival .
A :  Read the like
A :  Mm yeah . Yeah yeah , Fes yeah , I think I think you can do the Festival , you can just call like uh this there are different kind of again , how you call the Festival , so you can just give the full there is an too . So then you will get the all the caller and other things . If if you want to like read full text , then you can say like the full text , like those syntax and then it will speak till it finishes the text .
B :  I think , yeah yeah , definitely .
C :  Mm okay .
C :  Yeah .
C :  And it's in real time on a reasonable machine ?
A :  Yeah , it's real it's yeah . Diphone , yeah , yeah . That's why it's real time , like otherwise it's bit difficult and it's okay , you can easily understand , so it's intelligible , so . Uh Festival is good , like now it comes at all the Linux boxes also , so in so already in this
B :  It's based on the diphones diphones , right ?
B :  Yeah , yeah .
C :  Okay .
B :  Yeah , I think it's very good , yeah .
C :  Okay .
B :  Mm-hmm .
C :  'Cause the the diphone that I was tried to install it and there there are the voices like cat voice and these voices , this is not the one ? Because uh I don't know which which um what i what is running where uh if it's diphone or which uh approach this is .
A :  No the there are so many voices , like again
A :  Which voice you want to use .
A :  No , most of the the Festival , uh what you get on Linux machines , is diphone based only . So they supply few voices . Yeah yeah , yeah , just uh concatenation of the no , it's already there , if you just put li Festival , it comes wi because it's comes with direct Linux software and you know , it is open source . Festival is open
C :  Yes ? So it's uh just uh wha samples of the diphones , and it choo chooses
B :  So you've got it installed in your machine here or
C :  Yeah , on my on my laptop . Or here .
B :  Festival , right ?
B :  Okay . Oh . A and have you have you tried it ?
C :  Yeah , you can you can easily yes , I d I've tried , but there are some built-in like easy voices which you can y uh there is a default one completely synthetic , which you cannot understand at all , and th then there is a diphone synthesis I think it is the diphone synthesis from the some like something like cat w voice and somethi this is so-so , but it's this doesn't sound very nice , and we were playing at the uh summer school with them , actually with the authors , and they were playing us some other other approaches , and it it sounded like a human , it was great , but
A :  It's
B :  And ?
B :  So you can
A :  Yeah , cat voice , like there are like they they have few
B :  Uh-huh .
A :  Yeah , new voices like , yeah .
A :  Yeah , that's the difference that tha in views this unit selection . It's not diphones , you have really large inventory of the speech . Uh those voices , they're not yet released . Yeah . No no , it's free , they'll be releasing soon I guess . So maybe you can just check no , i it's it's here on the web , then it won't come with your Linux software , so you have to again download from uh their web page .
C :  Yeah .
C :  So this is not this is not for free or something .
C :  But it it's not here so far .
C :  Okay , I'm I'm fine with this , but I can make it run on my machine . Okay .
A :  Yeah , you can download , now . Yeah , yeah yeah . It the voices are called Multisyn , Multisyn uh sl uh that's some voice . So you can download from the voice from the web Festival .
C :  Okay .
C :  And and the engine is is in the Festival itself . So it's just a voice to to okay .
A :  Yeah yeah yeah , yeah . Yeah yeah , yeah . So voice is like the database of , the parallel speaker who speaks that this text , so . It was really large database , so it the voi the quality will be better because you can find similar yeah , it's more or less but not exactly like uh it's two times or one point five times or something . But you can prune like how much you want , because
C :  okay .
C :  Mm-hmm .
D :  So it's still still real time ?
D :  Hmm .
D :  Ah okay , but but not much , yeah .
C :  Mm mm .
D :  Yeah , yeah .
C :  Okay .
A :  so this is again like uh that's kind of my P_H_D_ work how in this system like how you choose the units and how you concatenate . So there are again cost functions and Viterbi . So you can you can always optimise , you can put some thresholds and pruning and then you can make it real time . So then again , compromise uh between the quality and yeah . But still it's good like now . Th you can just check Multisyn and then voices . Uh yeah , yeah , it's really good .
B :  Mm-hmm .
C :  Hmm .
B :  Yeah , it's very difficult and
C :  Mm-hmm .
B :  Yeah , yeah .
D :  Alright . Yeah , yeah .
C :  Hmm .
C :  Okay .
C :  Uh it's right , I mean
B :  And you just uh have chosen some text ? Any text you just put in and
A :  Yeah yeah yeah yeah .
C :  Yeah , I just s use the Douglas Adams , uh uh that that book , you know , the mm how it's called ? Just general English textbook , I I cho I tried .
A :  But only sometimes like it can't really call this yeah .
B :  Okay .
B :  And it can read uh how does it read , from which um from which uh how to state it ? Uh is it uh possible to read P_D_F_s or just standard text ? Whatever , T_X_T_ , right , files . Uh-huh .
C :  No , the T_ T_X_T_ . Yeah , ASCII file basically . You have to but you can take a P_D_F_ and convert P_D_F_ to ASCII .
A :  Uh standard text . Yeah . Yeah , ASCII file is
A :  ASCII and but only the problem's again acronyms and sometimes it expands , sometimes it may not expand , because it's not in their dictionary , so yeah . It's not in the dictionary , like if we again it's problem like , so . But those things you can add , like if you're really familiar with Festival , so you can add always these
B :  Yeah . Well let's
C :  Yeah . It was a line with hashes and it was hash hash hash hash hash hash hash hash hash hash .
B :  Oh really ?
C :  Uh I'm not , then I don't want to go too much into details , just a little bit .
A :  Yeah . But at least these things okay like , it won't really make mistakes so often like , sometimes
C :  Mm well I can prepare s sorry .
B :  Anyway , who is the author of Festival ?
A :  Alan Black and Paul Taylor , they started r Festival in ninety six or something . Edinburgh C_S_T_R_ , yeah .
C :  These are the Edinburgh people , no ?
B :  Edinburgh ? And I thought it that's the guy from C_S_L_U_ , from Portland , who was there during f
A :  Uh f oh uh this ma this Johann Waters or like no , they d what they did Portland O_G_I_ , they did L_P_C_ based synthesis . The Festival is alrea already like it's one first you start with th
B :  Yeah , but there was
B :  No , no , but that that was the guy who's on a wheelchair now , he cannot m move even , he's very in like indic how to say that ? Like he cannot have he just can speak only through some Festival system or something like that . He was the auth no , yeah , h he was working I think in w O_G_I_ even , he's very famous , but I thought that he was the author no , it was developed for him , like he was just first tester I think of the system . Something like that . Like it's not t text to speech , it's just the uh uh synthesis , which means which is more or less the same , but I dunno .
A :  Ah , okay .
A :  Mm .
A :  From .
A :  I don't know . No no , actually the b
A :  Ah , okay , okay . Yeah , maybe , because
A :  Ah , ok
A :  Hmm .
A :  Yeah , first they started in C_S_T_R_ with Alan Black and Paul Taylor , then then it expand to many places , because if somebody is fil no , Simon King is he's one of the others , but Rob Clark is the man that you met um . Uh uh , already . Yeah .
B :  Yeah . So it's somebody and Simon King is playing with that , right , the t guy .
C :  Mm-hmm , yeah , voice . That I know . Um uh .
B :  Okay , okay .
A :  It's
B :  I think that's enough .
D :  Yeah .
C :  N uh .
A :  So anything uh more about Lisbon or Interspeech ? I it's enough , yeah .
B :  No other papers it's just one paper and and Festival . From Linux , it's very im interesting .
D :  Okay .
C :  N uh .
A :  Okay .
B :  Yeah .
C :  Mm .
